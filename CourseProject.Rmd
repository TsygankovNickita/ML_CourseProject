---
title: "ML project"
author: "Tsygankov Nickita"
date: "05 06 2018"
output: html_document
---

# Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

## Aims 
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

## Preparation and reproducibility procuring
First of all we should load the data. If u haven't got one of this packages, please, install it before starting.
```{r}
library(caret) 
library(rpart)
library(randomForest)

set.seed(127) ## seed will guarantee reproducibility
```

## Cleaning and exploring the data
Our data have a problem wih NA's - they exist in diffrent formats, and we need exclude this problem on the stage of loading data
```{r}
training <- read.csv("pml-training.csv", na.strings=c("", "NA", "#DIV/0!"), row.names = 1)
testing <- read.csv("pml-testing.csv", na.strings=c("", "NA", "#DIV/0!"), row.names = 1)
training <- training[,!sapply(training,function(x) any(is.na(x)))]
testing <- testing[,!sapply(testing,function(x) any(is.na(x)))]
```
Several first columns contain information, which would be not useful for our analysis and can create noise for ML instruments. We should exclude them too.
```{r}
training <-training[,-c(1:6)]
testing <-testing[,-c(1:6)]
```

## Data spliting and training
We need cut our data for training and validation parts
```{r}
indices <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
Train <- training[indices, ] 
Validation <- training[-indices, ]
```
We have a large data set with about 20000 observation. This means that with high probability random forests should demonstrate best results.
```{r}
model_forests <- randomForest(classe ~ ., data = Train)
```
## Prediction 
Let's test our prediction model!
```{r}
pred_for <- predict(model_forests, Validation, type = "class")
confusionMatrix(pred_for, Validation$classe)
```
We got a few missing values, this fact means that our model works great! No we need to implement compare with confusion matrix of our first data part.
```{r}
pred_for_train <- predict(model_forests, Train, type = "class")
confusionMatrix(pred_for_train, Train$classe)
```
efreshingly, random forests does even better on the training data, as it should. Note that if we keep all variables, we get even better performance and appearance of good predictions on the validation data, but can’t predict on the test data when we get names of new people. So we’ve overfit to the names of the people. The performance is still quite good, though.

Given this, we expect near perfect accuracy on the test data (validation data in the data science specialization convention).
```{r}
pred_for_test <- predict(model_forests, testing, type = "class")
pred_for_test
```
The result supprased our expectations and we not need to use another models and instruments.

# Conclusion
Random forests predictably show a great perfomance on big data set and we got an exellent results with testing data. Thanks for attention! 
